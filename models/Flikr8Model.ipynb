{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Flikr8Model.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTX3Qge00DAS","executionInfo":{"status":"ok","timestamp":1638078503869,"user_tz":360,"elapsed":3165,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}},"outputId":"8a744408-435b-4897-faa2-513f8112ea99"},"source":["import tensorflow\n","print('tensorflow: %s' % tensorflow.__version__)\n","# keras version\n","import keras\n","print('keras: %s' % keras.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["tensorflow: 2.7.0\n","keras: 2.7.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whMsELcpBQq5","executionInfo":{"status":"ok","timestamp":1638078528015,"user_tz":360,"elapsed":22540,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}},"outputId":"4ffbdc36-d89d-46dd-a0c6-38d8d217ff72"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"C75PofHUBU4z","executionInfo":{"status":"ok","timestamp":1638078530186,"user_tz":360,"elapsed":551,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}}},"source":["import os\n","os.chdir('/content/drive/MyDrive/AC215 Project/dataset')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"TC4QCcEpF2Ig","executionInfo":{"status":"ok","timestamp":1638078580036,"user_tz":360,"elapsed":151,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}}},"source":["PREPROCESSING = False"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rh78YLIEBD5p","executionInfo":{"status":"ok","timestamp":1638078645167,"user_tz":360,"elapsed":185,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}}},"source":["from os import listdir\n","from pickle import dump\n","from keras.applications.vgg16 import VGG16\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.models import Model\n"," \n","# extract features from each photo in the directory\n","def extract_features(directory):\n","\t# load the model\n","\tmodel = VGG16()\n","\t# re-structure the model\n","\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","\t# summarize\n","\tprint(model.summary())\n","\t# extract features from each photo\n","\tfeatures = dict()\n","\tfor name in listdir(directory):\n","\t\t# load an image from file\n","\t\tfilename = directory + '/' + name\n","\t\timage = load_img(filename, target_size=(224, 224))\n","\t\t# convert the image pixels to a numpy array\n","\t\timage = img_to_array(image)\n","\t\t# reshape data for the model\n","\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\t\t# prepare the image for the VGG model\n","\t\timage = preprocess_input(image)\n","\t\t# get features\n","\t\tfeature = model.predict(image, verbose=0)\n","\t\t# get image id\n","\t\timage_id = name.split('.')[0]\n","\t\t# store feature\n","\t\tfeatures[image_id] = feature\n","\t\tprint('>%s' % name)\n","\treturn features\n","\n","if PREPROCESSING:\n","\t# extract features from all images\n","\tdirectory = 'Flickr8Dataset/Flicker8k_Dataset'\n","\tfeatures = extract_features(directory)\n","\tprint('Extracted Features: %d' % len(features))\n","\t# save to file\n","\tdump(features, open('features.pkl', 'wb'))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZdIehywIuvQ","executionInfo":{"status":"ok","timestamp":1638078688218,"user_tz":360,"elapsed":1532,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}},"outputId":"ebbb2f99-84fb-4e1a-b1ee-3339f4ad8e40"},"source":["import string\n"," \n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","# extract descriptions for images\n","def load_descriptions(doc):\n","\tmapping = dict()\n","\t# process lines\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\tif len(line) < 2:\n","\t\t\tcontinue\n","\t\t# take the first token as the image id, the rest as the description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# remove filename from image id\n","\t\timage_id = image_id.split('.')[0]\n","\t\t# convert description tokens back to string\n","\t\timage_desc = ' '.join(image_desc)\n","\t\t# create the list if needed\n","\t\tif image_id not in mapping:\n","\t\t\tmapping[image_id] = list()\n","\t\t# store description\n","\t\tmapping[image_id].append(image_desc)\n","\treturn mapping\n"," \n","def clean_descriptions(descriptions):\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor i in range(len(desc_list)):\n","\t\t\tdesc = desc_list[i]\n","\t\t\t# tokenize\n","\t\t\tdesc = desc.split()\n","\t\t\t# convert to lower case\n","\t\t\tdesc = [word.lower() for word in desc]\n","\t\t\t# remove punctuation from each token\n","\t\t\tdesc = [w.translate(table) for w in desc]\n","\t\t\t# remove hanging 's' and 'a'\n","\t\t\tdesc = [word for word in desc if len(word)>1]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tdesc = [word for word in desc if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tdesc_list[i] =  ' '.join(desc)\n"," \n","# convert the loaded descriptions into a vocabulary of words\n","def to_vocabulary(descriptions):\n","\t# build a list of all description strings\n","\tall_desc = set()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n","\treturn all_desc\n"," \n","# save descriptions to file, one per line\n","def save_descriptions(descriptions, filename):\n","\tlines = list()\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor desc in desc_list:\n","\t\t\tlines.append(key + ' ' + desc)\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n"," \n","filename = 'Flickr8Dataset/Flickr8k_text/Flickr8k.token.txt'\n","# load descriptions\n","doc = load_doc(filename)\n","# parse descriptions\n","descriptions = load_descriptions(doc)\n","print('Loaded: %d ' % len(descriptions))\n","# clean descriptions\n","clean_descriptions(descriptions)\n","# summarize vocabulary\n","vocabulary = to_vocabulary(descriptions)\n","print('Vocabulary Size: %d' % len(vocabulary))\n","# save to file\n","save_descriptions(descriptions, 'descriptions.txt')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded: 8092 \n","Vocabulary Size: 8763\n"]}]},{"cell_type":"code","metadata":{"id":"CfDgaG8eROG5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638082393061,"user_tz":360,"elapsed":3688075,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}},"outputId":"11f6ad43-62da-4ea2-92bc-feb032699edd"},"source":["from numpy import array\n","from pickle import load\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","from keras.layers.merge import add\n","from keras.callbacks import ModelCheckpoint\n"," \n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n"," \n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n"," \n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n"," \n","# covert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n"," \n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n"," \n","# create sequences of images, input sequences and output words for an image\n","def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n","\tX1, X2, y = list(), list(), list()\n","\t# walk through each description for the image\n","\tfor desc in desc_list:\n","\t\t# encode the sequence\n","\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n","\t\t# split one sequence into multiple X,y pairs\n","\t\tfor i in range(1, len(seq)):\n","\t\t\t# split into input and output pair\n","\t\t\tin_seq, out_seq = seq[:i], seq[i]\n","\t\t\t# pad input sequence\n","\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","\t\t\t# encode output sequence\n","\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\t\t\t# store\n","\t\t\tX1.append(photo)\n","\t\t\tX2.append(in_seq)\n","\t\t\ty.append(out_seq)\n","\treturn array(X1), array(X2), array(y)\n"," \n","# define the captioning model\n","def define_model(vocab_size, max_length):\n","\t# feature extractor model\n","\tinputs1 = Input(shape=(4096,))\n","\tfe1 = Dropout(0.5)(inputs1)\n","\tfe2 = Dense(256, activation='relu')(fe1)\n","\t# sequence model\n","\tinputs2 = Input(shape=(max_length,))\n","\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","\tse2 = Dropout(0.5)(se1)\n","\tse3 = LSTM(256)(se2)\n","\t# decoder model\n","\tdecoder1 = add([fe2, se3])\n","\tdecoder2 = Dense(256, activation='relu')(decoder1)\n","\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\t# tie it together [image, seq] [word]\n","\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\t# compile model\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n","\t# summarize model\n","\tmodel.summary()\n","\tplot_model(model, to_file='model.png', show_shapes=True)\n","\treturn model\n"," \n","# data generator, intended to be used in a call to model.fit_generator()\n","def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n","\t# loop for ever over images\n","\twhile 1:\n","\t\tfor key, desc_list in descriptions.items():\n","\t\t\t# retrieve the photo feature\n","\t\t\tphoto = photos[key][0]\n","\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n","\t\t\tyield [in_img, in_seq], out_word\n"," \n","# load training dataset (6K)\n","filename = 'Flickr8Dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('descriptions.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","# photo features\n","train_features = load_photo_features('features.pkl', train)\n","print('Photos: train=%d' % len(train_features))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)\n"," \n","# define the model\n","model = define_model(vocab_size, max_length)\n","# train the model, run epochs manually and save after each epoch\n","epochs = 4\n","steps = len(train_descriptions)\n","for i in range(epochs):\n","\t# create the data generator\n","\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n","\t# fit for one epoch\n","\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","\t# save model\n","\tmodel.save('model_' + str(i) + '.h5')"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: 6000\n","Descriptions: train=6000\n","Photos: train=6000\n","Vocabulary Size: 7579\n","Description Length: 34\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, 34)]         0           []                               \n","                                                                                                  \n"," input_1 (InputLayer)           [(None, 4096)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 34, 256)      1940224     ['input_2[0][0]']                \n","                                                                                                  \n"," dropout (Dropout)              (None, 4096)         0           ['input_1[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 34, 256)      0           ['embedding[0][0]']              \n","                                                                                                  \n"," dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n","                                                                                                  \n"," add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n","                                                                  'lstm[0][0]']                   \n","                                                                                                  \n"," dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n","                                                                                                  \n"," dense_2 (Dense)                (None, 7579)         1947803     ['dense_1[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,527,963\n","Trainable params: 5,527,963\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:170: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"stream","name":"stdout","text":["6000/6000 [==============================] - 919s 152ms/step - loss: 4.6851\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n"]},{"output_type":"stream","name":"stdout","text":["6000/6000 [==============================] - 907s 151ms/step - loss: 3.9130\n","6000/6000 [==============================] - 907s 151ms/step - loss: 3.6583\n","6000/6000 [==============================] - 905s 151ms/step - loss: 3.5032\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ki1etllYwczo","executionInfo":{"status":"ok","timestamp":1638083645023,"user_tz":360,"elapsed":662310,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}},"outputId":"42d36929-5c77-4dcf-f89c-aca5ce09b981"},"source":["from numpy import argmax\n","from pickle import load\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n"," \n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n"," \n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n"," \n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n"," \n","# covert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n"," \n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n"," \n","# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n"," \n","# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text\n"," \n","# evaluate the skill of the model\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc_list in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\treferences = [d.split() for d in desc_list]\n","\t\tactual.append(references)\n","\t\tpredicted.append(yhat.split())\n","\t# calculate BLEU score\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"," \n","# prepare tokenizer on train set\n"," \n","# load training dataset (6K)\n","filename = 'Flickr8Dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('descriptions.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)\n"," \n","# prepare test set\n"," \n","# load test set\n","filename = 'Flickr8Dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n","test = load_set(filename)\n","print('Dataset: %d' % len(test))\n","# descriptions\n","test_descriptions = load_clean_descriptions('descriptions.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","# photo features\n","test_features = load_photo_features('features.pkl', test)\n","print('Photos: test=%d' % len(test_features))\n"," \n","# load the model\n","filename = 'model_3.h5'\n","model = load_model(filename)\n","# evaluate model\n","evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: 6000\n","Descriptions: train=6000\n","Vocabulary Size: 7579\n","Description Length: 34\n","Dataset: 1000\n","Descriptions: test=1000\n","Photos: test=1000\n","BLEU-1: 0.530407\n","BLEU-2: 0.284449\n","BLEU-3: 0.194158\n","BLEU-4: 0.093164\n"]}]},{"cell_type":"code","metadata":{"id":"0XK9i3eMGiHg","executionInfo":{"status":"ok","timestamp":1638082969187,"user_tz":360,"elapsed":138,"user":{"displayName":"anita mahinpei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10893440274338475761"}}},"source":["dump(tokenizer, open('tokenizer.pkl', 'wb'))"],"execution_count":13,"outputs":[]}]}